{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4628b970-a65b-4a47-aa6a-439f8f4d168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "\n",
    "import h5py\n",
    "\n",
    "from unidecode import unidecode \n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc4955-9e7a-4e22-85a3-2aa31da75398",
   "metadata": {},
   "source": [
    "## Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ebda5-0ea4-4195-8155-421e31a41712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# Lien de téléchargement direct du fichier Google Drive\n",
    "url = 'https://drive.google.com/uc?id=19CI6Py6HZ2f9p4eQiThFLi9IOWHYDvcs'\n",
    "\n",
    "# Chemin de destination pour sauvegarder le fichier téléchargé\n",
    "output = 'data.zip' \n",
    "\n",
    "# Télécharger le fichier\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2833045-db1a-46cd-adb0-31fdf78d78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Nom du fichier zip téléchargé\n",
    "zip_file = \"data.zip\"  \n",
    "\n",
    "# Répertoire de destination pour extraire les fichiers\n",
    "extract_dir = \".\"  # Remplacez par le répertoire de destination souhaité\n",
    "\n",
    "# Extraire le fichier zip\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "# Supprimer le fichier zip une fois qu'il a été extrait\n",
    "os.remove(zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f2e8b-5fed-4731-944b-7b12b30d7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer les fichiers CSV\n",
    "os.rename('X_train_update.csv', 'X_train.csv')\n",
    "os.rename('X_test_update.csv', 'X_test.csv')\n",
    "os.rename('Y_train_CVw08PX.csv', 'Y_train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b31758-7984-4b94-bdd6-f3c3e08e2649",
   "metadata": {},
   "source": [
    "## Pré - traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf396086-e1f5-4cab-96c4-9e45a69e7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répertoire Images \n",
    "folder_img = 'images/images/image_train/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89fae7db-1c9b-4202-b630-c2e525ec531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 2 CSVs\n",
    "X = pd.read_csv('X_train.csv', delimiter=',', index_col=0)\n",
    "y = pd.read_csv('Y_train.csv', delimiter=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a247d6a4-1b71-46e7-be3c-4dcfbfd268c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met de côté un jeu de test auquel on ne touchera pas jusqu'à la fin, au moment de mesurer la performance du modèle retenu\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b514c09-3d2f-4292-94ce-607237de192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocessing(img_filename):\n",
    "\n",
    "    \"\"\"\n",
    "    - Recadre l'image en lui retirant ses marges blanches si elle en a ;\n",
    "    - Redimensionne l'image en 100 x 100 ;\n",
    "    - Transforme l'image en niveaux de gris.\n",
    "    \"\"\"\n",
    "    \n",
    "    width = 100\n",
    "    height = 100\n",
    "\n",
    "    # Lecture de l'image avec OpenCV\n",
    "    img = cv2.imread(folder_img + img_filename)\n",
    "    \n",
    "    # Conversion de l'image en niveaux de gris\n",
    "    image_gris = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Inversion des niveaux de gris\n",
    "    image_inversee = cv2.bitwise_not(image_gris)\n",
    "\n",
    "    # Recherche des contours dans l'image\n",
    "    contours, hierarchy = cv2.findContours(image_inversee, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        \n",
    "        # Recherche du contour le plus externe\n",
    "        contour_externe = max(contours, key=cv2.contourArea)\n",
    "\n",
    "        # Recherche des coordonnées du rectangle englobant le contour externe\n",
    "        x_min, y_min, w, h = cv2.boundingRect(contour_externe)\n",
    "\n",
    "        # Rognement de l'image en utilisant les coordonnées du rectangle englobant\n",
    "        #cropped_image = image_gris[y_min:y_min+h, x_min:x_min+w]\n",
    "        #Ou pour des images en couleurs :\n",
    "        cropped_image = img[y_min:y_min+h, x_min:x_min+w]\n",
    "        cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Redimensionnement de l'image pour qu'elle ait la même taille que les autres\n",
    "        cropped_image_resized = cv2.resize(cropped_image, (width, height))\n",
    "\n",
    "        return np.array(cropped_image_resized)\n",
    "\n",
    "    else:\n",
    "        # Si aucun contour n'est trouvé, on se contente de redimensionner l'image\n",
    "        #image_resized = cv2.resize(image_gris, (width, height))\n",
    "        #Ou pour des images en couleurs :\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        image_resized = cv2.resize(img, (width, height))\n",
    "        return np.array(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfdba58-21a3-41e0-8e95-cda8de7e69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_filenames = [f\"image_{X.loc[idx, 'imageid']}_product_{X.loc[idx, 'productid']}.jpg\" for idx in X.index]\n",
    "\n",
    "processed_images = []\n",
    "\n",
    "for img_filename in img_filenames:\n",
    "    # Recadrage, redimensionnement et transformation en gris (cf. fonction définie plus haut)\n",
    "    img = img_preprocessing(img_filename)\n",
    "    processed_images.append(img)\n",
    "\n",
    "processed_images = np.array(processed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13cd553c-83f6-43ea-a5e4-997d91ee767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_txt = X.loc[:, ['designation', 'description']]\n",
    "\n",
    "# Merging the column designation with the description and put everything in lowercase\n",
    "X_txt[\"text\"] = X_txt['designation'].fillna('').str.lower() + ' ' + X_txt['description'].fillna('').str.lower()\n",
    "\n",
    "# Cleaning the dataframe by dropping the columns which are not useful anymore\n",
    "X_txt.drop(['designation', 'description'], axis=1, inplace = True)\n",
    "\n",
    "# Cleaning the text\n",
    "# Deleting special character and accent with unidecode\n",
    "X_txt['text'] = X_txt['text'].apply(unidecode).astype('str')\n",
    "# Deleting HTML code\n",
    "X_txt['text'] = X_txt['text'].str.replace(r'<[^<>]*>', '', regex=True)\n",
    "# Tokenisation et deleting words with less than 3 letters\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z-]{3,}\")\n",
    "X_txt['text'] = X_txt['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "#Deleting the stop words\n",
    "stop_words = set(stopwords.words(['english','french','german']))\n",
    "# Adding in addition to the stop words, the words useless for us\n",
    "parasite_words_words = ['plus', 'peut', 'etre', 'tout', 'cette', 'tres']\n",
    "html_code_words = ['rsquo', 'eacute', 'agrave', 'egrave', 'div', 'span', 'class', 'nbsp', 'amp', 'ecirc', 'ccedil', 'laquo', 'raquo']\n",
    "stop_words.update(parasite_words_words)\n",
    "stop_words.update(html_code_words)\n",
    "# Function to delete stop words from our DF\n",
    "def stop_words_filtering(mots) :\n",
    "    tokens = []\n",
    "    for mot in mots:\n",
    "        if mot not in stop_words:  \n",
    "            tokens.append(mot)\n",
    "    return tokens\n",
    "#Deleting stop words from our DF using our function\n",
    "X_txt[\"text\"] = X_txt[\"text\"].apply(stop_words_filtering)\n",
    "\n",
    "# Initialiser le lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Fonction pour lemmatiser une liste de tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "X_txt['text'] = X_txt['text'].apply(lemmatize_tokens)\n",
    "X_txt = X_txt['text'].apply(lambda x: ' '.join(x)).astype(str).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1ab3b-dc11-46da-b3b6-774902088078",
   "metadata": {},
   "source": [
    "## Création du fichier processed_data.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08feaaaa-70fb-4611-8bc8-330e7a8c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un fichier HDF5 et y enregistrer un tableau NumPy\n",
    "with h5py.File('processed_data.h5', 'w') as hf:\n",
    "    hf.create_dataset('img', data=processed_images, compression='gzip')\n",
    "    hf.create_dataset('image_id', data=np.array(X['imageid']), compression='gzip')\n",
    "    hf.create_dataset('product_id', data=np.array(X['productid']), compression='gzip')\n",
    "    hf.create_dataset('label', data=np.array(y['prdtypecode']), compression='gzip')\n",
    "    hf.create_dataset('text', data=X_txt, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202c2f7-9c64-418c-a495-f04324a637bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mon_env5",
   "language": "python",
   "name": "mon_env5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
