{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec4483d",
   "metadata": {},
   "source": [
    "# Combinaison des Textes et images dans un réseau de neuronnes bimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60abb226",
   "metadata": {},
   "source": [
    "## Intégration des données et traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5324af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the 2 CSVs\n",
    "X = pd.read_csv( 'X_train.csv', delimiter=',', index_col=0)\n",
    "y = pd.read_csv( 'Y_train.csv', delimiter=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50fc5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met de côté un jeu de test auquel on ne touchera pas jusqu'à la fin, au moment de mesurer la performance du modèle retenu\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e17e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['img_path'] = 'image_' + X['imageid'].astype(str) + '_product_' + X['productid'].astype(str) + '.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be50a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Nombre total de catégories\n",
    "num_categories = len(label_encoder.classes_)\n",
    "\n",
    "# Convertir en représentation one-hot\n",
    "y_one_hot = to_categorical(y_encoded, num_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd816f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, \n",
    "                                                          test_size=0.2, \n",
    "                                                          random_state=0, \n",
    "                                                          stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff209cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = 27\n",
    "max_words = 10000\n",
    "max_len = 34          # correspond au nombre de mots maximum du jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda348d8",
   "metadata": {},
   "source": [
    "## Génération des images cropping et size 100x100x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5634eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocessing(img_filename, input_dir, width=100, height=100, cropping=True):\n",
    "\n",
    "    \"\"\"\n",
    "    - Par défaut, recadre l'image en lui retirant ses marges blanches si elle en a ;\n",
    "    - Redimensionne l'image en 224 x 224 par défaut, ou selon les dimensions passées en arguments 'width' et 'height'\n",
    "    \"\"\"\n",
    "\n",
    "    # Lecture de l'image avec OpenCV\n",
    "    img = cv2.imread(input_dir + img_filename)\n",
    "\n",
    "    if cropping:\n",
    "        # Conversion de l'image en niveaux de gris\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Inversion des niveaux de gris\n",
    "        img_reversed = cv2.bitwise_not(img_gray)\n",
    "    \n",
    "        # Recherche des contours dans l'image\n",
    "        contours, hierarchy = cv2.findContours(img_reversed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "        if contours:\n",
    "            # Recherche du contour le plus externe\n",
    "            contour_external = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "            # Recherche des coordonnées du rectangle englobant le contour externe\n",
    "            x_min, y_min, w, h = cv2.boundingRect(contour_external)\n",
    "    \n",
    "            # Rognement de l'image en utilisant les coordonnées du rectangle englobant\n",
    "            img = img[y_min:y_min+h, x_min:x_min+w]\n",
    "\n",
    "    # Redimensionnement de l'image\n",
    "    img_resized = cv2.resize(img, (width, height))\n",
    "\n",
    "    return img_resized;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46992e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "folder_img = 'C:/Users/Nans/Documents/Rakuten Project/images/images/image_train/'\n",
    "x_img = []\n",
    "\n",
    "for idx, item in enumerate(X_train['img_path'].values):\n",
    "    x_img.append(img_preprocessing(item, folder_img))\n",
    "    sys.stdout.write(\"\\rProgression : {}/{}\".format(idx+1, len(X_train['img_path'].values)))\n",
    "    sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_img_val = []\n",
    "\n",
    "for idx, item in enumerate(X_val['img_path'].values):\n",
    "    x_img_val.append(img_preprocessing(item, folder_img))\n",
    "    sys.stdout.write(\"\\rProgression : {}/{}\".format(idx+1, len(X_val['img_path'].values)))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9343a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3334d10a",
   "metadata": {},
   "source": [
    "## Création du tokenizer et du modèle bimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on importe le tokenizer créé dans la partie deep learning texte\n",
    "import pickle\n",
    "\n",
    "# Charger le tokenizer avec pickle\n",
    "with open(\"../models/tokenizer.pkl\", \"rb\") as f:\n",
    "    tok = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du modèle pour le texte (embedding)\n",
    "max_seq_length = 34  # Longueur maximale de la séquence de texte\n",
    "vocab_size = 10000   # Taille du vocabulaire\n",
    "embedding_dim = 100  # Dimension de l'embedding\n",
    "\n",
    "# pour la partie train\n",
    "#tok = Tokenizer(num_words=vocab_size)\n",
    "#tok.fit_on_texts(X_train['designation'])\n",
    "sequences = tok.texts_to_sequences(X_train['designation'])\n",
    "sequences_matrix = pad_sequences(sequences,maxlen=max_seq_length)\n",
    "\n",
    "# pour la partie test\n",
    "test_sequences = tok.texts_to_sequences(X_val['designation'])\n",
    "test_sequences_matrix = pad_sequences(test_sequences,maxlen=max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5ebe0-6311-47e0-9945-83d4fbb6abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le tableau en HDF5\n",
    "with h5py.File('xy_val_100.h5', 'w') as f:\n",
    "    f.create_dataset('x_img_val', data=x_img_val)\n",
    "    f.create_dataset('test_sequences_matrix', data=test_sequences_matrix)\n",
    "    f.create_dataset('y_val', data=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping( patience=2, # Attendre n epochs avant application\n",
    "                                min_delta = 0.01, # si au bout de n epochs la fonction de perte ne varie pas de 1%, \n",
    "                                verbose=1, # Afficher à quel epoch on s'arrête\n",
    "                                mode = 'min',\n",
    "                                monitor='val_loss')\n",
    "\n",
    "# Learning rate\n",
    "reduce_learning_rate = ReduceLROnPlateau(\n",
    "                                    monitor=\"val_loss\",\n",
    "                                    patience=2, #si val_loss stagne sur n epochs consécutives selon la valeur min_delta\n",
    "                                    min_delta= 0.01,\n",
    "                                    factor=0.5,  # On réduit le learning rate d'un facteur x\n",
    "                                    cooldown = 4, # On attend n epochs avant de réitérer \n",
    "                                    verbose=1)\n",
    "\n",
    "# on compile nos callbacks\n",
    "callbacks_list = [early_stopping, reduce_learning_rate]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input, Embedding, Concatenate\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Définition du modèle pour les images (VGG16 pré-entraîné)\n",
    "image_input = Input(shape=(100, 100, 3))\n",
    "img_model = VGG16(weights='imagenet', include_top=False, input_tensor=image_input)\n",
    "image_features = Flatten()(img_model.output)\n",
    "\n",
    "# Définition du modèle pour le texte\n",
    "text_input = Input(shape=(max_seq_length,))\n",
    "text_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input)\n",
    "text_features = Flatten()(text_embedding)\n",
    "\n",
    "# Concaténation des caractéristiques des deux modalités\n",
    "merged_features = Concatenate()([image_features, text_features])\n",
    "\n",
    "# Couche dense pour la classification finale\n",
    "output = Dense(27, activation='softmax')(merged_features)\n",
    "\n",
    "# Création du modèle multimodal\n",
    "model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Afficher un résumé du modèle\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2c71f",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle en utilisant vos données\n",
    "model.fit(x=[X_img, sequences_matrix], \n",
    "          y=y_train, \n",
    "          epochs=10, \n",
    "          batch_size=16, \n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=([X_img_val, test_sequences_matrix], y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72aa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "accr = model.evaluate([X_img_val, test_sequences_matrix], y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86648d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le chemin vers la sauvegarde du réseau\n",
    "model_savepath  = '../models/combi.h5'\n",
    "\n",
    "# Sauvegarde du réseau après entrainement\n",
    "model.save(model_savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dcabf9",
   "metadata": {},
   "source": [
    "## Validation du modèle et résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_savepath  = '../models/combi.h5'\n",
    "\n",
    "# Charger le modèle\n",
    "model = load_model(model_savepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc241adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([X_img_val, test_sequences_matrix])\n",
    "\n",
    "y_pred = np.argmax(pred, axis = -1)\n",
    "y_true = np.argmax(y_val, axis = -1)\n",
    "\n",
    "cm = tf.math.confusion_matrix(labels=y_true, predictions=y_pred).numpy()\n",
    "normalized_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c28201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(np.array(y_true), np.array(y_pred), target_names=list(map(str,label_encoder.classes_)))\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce16185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(normalized_cm, annot=True, cmap='Blues', fmt='.2f', \n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "\n",
    "plt.title('Matrice de confusion normalisée')\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Valeurs réelles')\n",
    "\n",
    "# Rotation des étiquettes des axes pour éviter les chevauchements\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "\n",
    "# Ajustement de l'espacement pour que tout soit bien visible\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e35144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rakuten_tf",
   "language": "python",
   "name": "rakuten_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
